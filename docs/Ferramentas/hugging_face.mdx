---
id: huggingface-colab-tutorial
title: 'Run Hugging Face Models in Google Colab'
sidebar_label: 'Hugging Face + Colab'
sidebar_position: 6
slug: /huggingface-colab
description: Step-by-step tutorial to open Hugging Face models in a Google Colab notebook with a single click.
keywords:
  - hugging face
  - google colab
  - AI
  - notebook
  - Colab
---


import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

## Overview

Google Colab and Hugging Face **have added an “Open&nbsp;in&nbsp;Colab” button to all _model cards_** on the Hub.
With a single click, you can generate a pre-configured notebook to **load, test, and even _fine-tune_** any model—skipping boilerplate and accelerating experiments. ([medium.com](https://medium.com/google-colab/launch-hugging-face-models-in-colab-for-faster-ai-exploration-bee261978cf9))

## Prerequisites

- Google account to access **[Colab](https://colab.research.google.com)**
- Hugging Face account (optional, but recommended for saving _tokens_ and private models)
- An up-to-date browser

:::tip GOOD TO KNOW
If you are a model author, simply include a `notebook.ipynb` file in the root of the repository, and it will be used instead of the automatically generated notebook. ([medium.com](https://medium.com/google-colab/launch-hugging-face-models-in-colab-for-faster-ai-exploration-bee261978cf9))
:::

## Step-by-Step

### 1. Choose the model

Go to any _model card_ on the Hub, for example:

```
https://huggingface.co/google/gemma-3-27b-it
```

### 2. Open in Colab

Click on **Use this model → Open in Colab**
or simply add `/colab` to the end of the URL:

```
https://huggingface.co/google/gemma-3-27b-it/colab
```

### 3. Configure the environment

In the generated notebook, go to **Runtime ▸ Change runtime type** and select **GPU** (or **TPU** if available).

> Free GPUs in Colab are sufficient for *inference* on most Transformer-based models.

### 4. Execute the cells

The first cell usually installs dependencies:

```python
!pip install -q transformers accelerate
```

Next, try out the model:

```python
from transformers import pipeline
pipe = pipeline("text-generation", model="google/gemma-3-27b-it")
print(pipe("Hello, how are you?")[0]['generated_text'])
```

### 5. Customize

- **Replace `model=`** with another Hugging Face model ID
- Adjust parameters like `max_new_tokens`, `temperature`, `top_p`
- Perform _fine-tuning_ by adding your own training step

<Tabs>
<TabItem value="CLI" label="Via CLI">

```bash
pip install huggingface_hub
export MODEL_ID=google/gemma-3-27b-it
python - <<'PY'
from huggingface_hub import InferenceClient
client = InferenceClient(model="$MODEL_ID")
print(client.text_generation("Hello, world!"))
PY
```

</TabItem>
<TabItem value="Python" label="Python">

```python
from huggingface_hub import InferenceClient
client = InferenceClient(model="google/gemma-3-27b-it")
client.text_generation("Hello, world!")
```

</TabItem>
</Tabs>

## For Model Authors

1. Create or upload a `notebook.ipynb` file demonstrating advanced usage of your model.
2. _Commit_ it to the same repository; the Hub will prioritize this file over the auto-generated one.
3. The “Open&nbsp;in&nbsp;Colab” button will now point to your custom notebook.

## Next Steps

- Explore **[Hugging Face Spaces](https://huggingface.co/spaces)** for interactive demos
- Integrate Colab with **[Kaggle Datasets](https://www.kaggle.com/datasets)** for large datasets
- Read the official announcement on Medium to understand the full motivation. ([medium.com](https://medium.com/google-colab/launch-hugging-face-models-in-colab-for-faster-ai-exploration-bee261978cf9))

## Resources

- [Medium Announcement](https://medium.com/google-colab/launch-hugging-face-models-in-colab-for-faster-ai-exploration-bee261978cf9)
- [Hugging Face Hub Documentation](https://huggingface.co/docs/huggingface_hub)
- [Transformers Quick Start](https://huggingface.co/docs/transformers/quickstart)
- [Colab FAQ](https://research.google.com/colaboratory/faq.html)

---
*Tutorial created on June 8, 2025, based on the Google Colab article*.
```